{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install requests beautifulsoup4\n",
        "\n",
        "import aiohttp\n",
        "import asyncio\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import nest_asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_VHJ1zlu2mv",
        "outputId": "e34503b4-7876-4cc4-8a6b-f5fff76d2ae2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.12.14)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_pehbVwDlY2Y",
        "outputId": "50028a9f-a3f8-49d5-ada2-3c1ba0de5f15"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the exact phrase to search for: Hutang\n",
            "Enter the maximum number of pages to search: 2\n",
            "Fetching page 1...\n",
            "Page 1: 12 articles added.\n",
            "Fetching page 2...\n",
            "Page 2: 12 articles added.\n",
            "\n",
            "Total articles processed: 24\n",
            "Results saved to Hutang_articles.csv\n"
          ]
        }
      ],
      "source": [
        "nest_asyncio.apply()  # Allow nested event loops in Jupyter Notebook\n",
        "\n",
        "# Prompt user to input the keyword\n",
        "keyword = input(\"Enter the exact phrase to search for: \").strip()\n",
        "\n",
        "# Prompt user to input the maximum number of pages to search\n",
        "max_pages = int(input(\"Enter the maximum number of pages to search: \"))\n",
        "\n",
        "# Set the base URL for search results on CNBC Indonesia\n",
        "base_url = f'https://www.cnbcindonesia.com/search/?query={keyword.replace(\" \", \"+\")}&page='\n",
        "\n",
        "# Function to fetch page content asynchronously\n",
        "async def fetch(session, url):\n",
        "    try:\n",
        "        async with session.get(url) as response:\n",
        "            return await response.text()\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Function to extract the article's publication date from the article page's meta tag\n",
        "async def extract_article_date(session, article_url):\n",
        "    try:\n",
        "        article_response = await fetch(session, article_url)\n",
        "        article_soup = BeautifulSoup(article_response, 'html.parser')\n",
        "        meta_tag = article_soup.find('meta', attrs={'name': 'publishdate'})\n",
        "        if meta_tag:\n",
        "            return meta_tag.get('content')\n",
        "    except Exception as e:\n",
        "        print(f\"Error extracting date for {article_url}: {e}\")\n",
        "    return None\n",
        "\n",
        "# Function to check if the exact phrase exists in the article body\n",
        "async def contains_exact_phrase(session, article_url, phrase):\n",
        "    try:\n",
        "        article_response = await fetch(session, article_url)\n",
        "        article_soup = BeautifulSoup(article_response, 'html.parser')\n",
        "        content = article_soup.find('main', class_='mx-auto w-full max-w-6xl flex-1 py-9')\n",
        "        if content:\n",
        "            return phrase.lower() in content.get_text(strip=True).lower()\n",
        "    except Exception as e:\n",
        "        print(f\"Error checking phrase for {article_url}: {e}\")\n",
        "    return False\n",
        "\n",
        "# Function to process a single article\n",
        "async def process_article(session, article, phrase, writer):\n",
        "    try:\n",
        "        link = article.get('href')\n",
        "        if not link.startswith('http'):\n",
        "            link = 'https://www.cnbcindonesia.com' + link\n",
        "\n",
        "        if await contains_exact_phrase(session, link, phrase):\n",
        "            article_response = await fetch(session, link)\n",
        "            article_soup = BeautifulSoup(article_response, 'html.parser')\n",
        "            title_tag = article_soup.find('h1', class_='mb-4 text-32 font-extrabold')\n",
        "            if title_tag:\n",
        "                title = title_tag.get_text(strip=True)\n",
        "                article_date = await extract_article_date(session, link)\n",
        "                writer.writerow([title, link, article_date])\n",
        "                return True\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing article: {e}\")\n",
        "    return False\n",
        "\n",
        "# Main function to handle asynchronous scraping\n",
        "async def main():\n",
        "    csv_filename = f'{keyword.replace(\" \", \"_\")}_articles.csv'\n",
        "    total_articles = 0\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        with open(csv_filename, mode='w', newline='', encoding='utf-8') as file:\n",
        "            writer = csv.writer(file)\n",
        "            writer.writerow(['Title', 'Link', 'Published On'])\n",
        "\n",
        "            for page_num in range(1, max_pages + 1):\n",
        "                url = base_url + str(page_num)\n",
        "                print(f\"Fetching page {page_num}...\")\n",
        "\n",
        "                try:\n",
        "                    page_content = await fetch(session, url)\n",
        "                    soup = BeautifulSoup(page_content, 'html.parser')\n",
        "                    articles = soup.find_all('a', href=True)\n",
        "\n",
        "                    # Stop loop if no articles are found\n",
        "                    if not articles:\n",
        "                        print(f\"No articles found on page {page_num}. Stopping...\")\n",
        "                        break\n",
        "\n",
        "                    tasks = [process_article(session, article, keyword, writer) for article in articles]\n",
        "                    results = await asyncio.gather(*tasks)\n",
        "\n",
        "                    articles_added = sum(results)\n",
        "                    total_articles += articles_added\n",
        "\n",
        "                    print(f\"Page {page_num}: {articles_added} articles added.\")\n",
        "\n",
        "                    # Stop if no articles were added from this page\n",
        "                    if articles_added == 0:\n",
        "                        print(f\"No matching articles added from page {page_num}. Stopping...\")\n",
        "                        break\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error fetching page {page_num}: {e}\")\n",
        "\n",
        "    print(f\"\\nTotal articles processed: {total_articles}\")\n",
        "    print(f\"Results saved to {csv_filename}\")\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == '__main__':\n",
        "    loop = asyncio.get_event_loop()\n",
        "    loop.run_until_complete(main())"
      ]
    }
  ]
}